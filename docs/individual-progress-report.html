<h1 id="ee-6770-final-project-individual-progress-report">EE 6770 – Final Project: Individual Progress Report</h1>
<p><strong>Student:</strong> [Your Name] <strong>Project:</strong> NTHU Driver Drowsiness Detection with ROI <strong>Date:</strong> November 19, 2025</p>
<hr />
<h2 id="part-1-technical-contribution-learning-70">Part 1: Technical Contribution &amp; Learning (70%)</h2>
<ul>
<li><strong>Main Roles:</strong>
<ul>
<li>Data prep validation (subject-exclusive splits under <code>data/splits/</code>).</li>
<li>Baseline modeling (ResNet50) and training orchestration in Colab.</li>
<li>Evaluation tooling (metrics/plots) and experiment reporting.</li>
<li>Runtime analysis and mitigation for time-consuming Colab training.</li>
</ul></li>
<li><strong>Implemented &amp; Tested:</strong>
<ul>
<li><code>src/eval/evaluate_model.py</code> (new CLI):
<ul>
<li>Computes accuracy, macro precision/recall/F1, per-class metrics.</li>
<li>Saves confusion matrix CSV and PNGs (raw + normalized), ROC curves (per class) with AUC.</li>
<li>Exports <code>predictions_test.csv</code> with probabilities per class.</li>
</ul></li>
<li>Colab workflow (<code>notebooks/colab_gpu_training.ipynb</code>) upgrades:
<ul>
<li>Regularized ResNet50 config writer (freeze backbone, dropout 0.7, LR 5e-5, WD 5e-4, patience 5, workers 2).</li>
<li>Train/eval/export cells for Regularized ResNet50 and EfficientNet-B0.</li>
<li>Comparison cell to plot Accuracy and Macro-F1 across baselines.</li>
<li>Speed-up &amp; resume cells: copy dataset to VM SSD, patch <code>num_workers: 2</code>.</li>
</ul></li>
<li>Reports:
<ul>
<li><code>docs/training-report-nov18-colab.md</code>: training status, overfitting analysis, timings, test results, next steps.</li>
<li><code>docs/report-2025-11-18.md</code>: daily summary and decision to stop training (time + generalization).</li>
</ul></li>
</ul></li>
<li><strong>Evidence (Code, Models, Figures):</strong>
<ul>
<li>Checkpoints: <code>checkpoints/baseline_resnet50_best.pth</code> (epoch 1), <code>baseline_resnet50_last.pth</code>.</li>
<li>Test (ResNet50 best): Accuracy 0.5818; Macro-F1 0.5806; ROC AUC (macro) 0.6415.</li>
<li>Generated figures (copied to <code>reports/figures/</code>):
<ul>
<li><code>confusion_matrix_test.png</code>, <code>confusion_matrix_test_normalized.png</code>, <code>roc_curves_test.png</code>.</li>
</ul></li>
<li>(If run) <code>comparison_baselines.png</code> (Macro-F1 vs Accuracy bar charts).</li>
<li>Minimal code snippet (evaluation entry):</li>
</ul>
<pre><code># src/eval/evaluate_model.py (excerpt)
metrics, cls_report, probs, labels = evaluate(model, test_loader, device=device, save_preds_path=preds_csv)
# Save metrics JSON + AUC
metrics_to_save = {k: (v.tolist() if hasattr(v, &quot;tolist&quot;) else v) for k, v in metrics.items()}
metrics_to_save[&quot;roc_auc&quot;] = roc_auc_scores</code></pre></li>
<li><strong>Challenge &amp; Resolution:</strong>
<ul>
<li>Challenge: Training on Colab became time-consuming with dataset streamed from Google Drive and <code>num_workers=4</code> (~5.25 s/iteration mid-epoch), while validation degraded after epoch 1 (classic overfitting).</li>
<li>Resolution: Copy dataset to local VM path (<code>datasets/archive</code>), reduce <code>num_workers</code> to 2, keep <code>pin_memory: true</code>. Adopt stronger regularization (freeze backbone, higher dropout, lower LR, higher WD) and add a lighter baseline (EfficientNet-B0) for faster epochs and better generalization.</li>
</ul></li>
</ul>
<h3 id="overfitting-analysis">Overfitting Analysis</h3>
<table>
<thead>
<tr class="header">
<th>Epoch</th>
<th style="text-align: right;">Train Loss</th>
<th style="text-align: right;">Train Macro-F1</th>
<th style="text-align: right;">Val Loss</th>
<th style="text-align: right;">Val Macro-F1</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td style="text-align: right;">0.2207</td>
<td style="text-align: right;">0.9071</td>
<td style="text-align: right;">0.8263</td>
<td style="text-align: right;"><strong>0.5772</strong></td>
<td>Best validation; checkpoint saved</td>
</tr>
<tr class="even">
<td>2</td>
<td style="text-align: right;">0.1218</td>
<td style="text-align: right;">0.9524</td>
<td style="text-align: right;">1.2085</td>
<td style="text-align: right;">0.4987</td>
<td>Validation performance drops sharply</td>
</tr>
<tr class="odd">
<td>3</td>
<td style="text-align: right;">0.1012</td>
<td style="text-align: right;">0.9602</td>
<td style="text-align: right;">1.4190</td>
<td style="text-align: right;">0.4782</td>
<td>Further decline; overfitting intensifies</td>
</tr>
</tbody>
</table>
<p>Pattern: Training metrics improve (loss ↓, F1 ↑) while validation loss increases and Macro-F1 decreases after epoch 1. This divergence indicates memorization of training samples rather than generalizable features.</p>
<p>Mitigation steps now in place: backbone freezing, increased dropout, stronger weight decay, lower learning rate, early stopping patience reduction, lighter alternative architecture (EfficientNet-B0), and improved data loading throughput (local copy + fewer workers).</p>
<p><strong>Challenge: Training Time (Bold)</strong> – In addition to overfitting, the throughput slowdown (≈5.25 s/iteration mid-epoch when reading from Drive with 4 workers) inflated epoch duration, making rapid experimentation impractical until mitigations (local copy, fewer workers) were applied.</p>
<hr />
<h2 id="part-2-plan-to-complete-before-final-presentation">Part 2: Plan to Complete Before Final Presentation</h2>
<ul>
<li>Near-term experiments (this week):
<ul>
<li>Train Regularized ResNet50 (Colab, local dataset copy, 2 workers); evaluate and export artifacts.</li>
<li>Train EfficientNet-B0; evaluate; run comparison plots (<code>reports/figures/comparison_baselines.png</code>).</li>
</ul></li>
<li>ROI phase: Train ROI-gated approach (<code>src/models/roi_gating.py</code>, <code>unet_segmentation.py</code>) focusing on eyes/mouth.</li>
<li>Reporting: Consolidate metrics/plots, per-class breakdowns; finalize slides.</li>
<li>Practical Colab steps:
<ul>
<li>Copy dataset to <code>/content/nthu-driver-drowsiness-ROI/datasets/archive</code>, patch <code>num_workers: 2</code>, run <code>--device cuda</code>.</li>
</ul></li>
<li>Milestones:
<ul>
<li>Week 1 (Nov 19–24): Complete regularized + EfficientNet runs and comparisons.</li>
<li>Week 2: ROI training/evaluation; finalize analysis and slides.</li>
</ul></li>
</ul>
<hr />
<h2 id="project-snapshot">Project Snapshot</h2>
<ul>
<li>Key configs: <code>configs/baseline_resnet50.yaml</code>, <code>configs/baseline_resnet50_regularized.yaml</code>, <code>configs/baseline_efficientnet.yaml</code>.</li>
<li>Relevant code: <code>src/training/train_baseline.py</code>, <code>src/training/trainer.py</code>, <code>src/data/dataset.py</code>, <code>src/data/transforms.py</code>, <code>src/eval/evaluate_model.py</code>.</li>
<li>Notebook: <code>notebooks/colab_gpu_training.ipynb</code> (GPU setup → train → evaluate → export → compare → speed-up/resume).</li>
<li>Figures: under <code>reports/figures/</code> (confusion matrices, ROC, comparison).</li>
</ul>
<hr />
<h2 id="colab-time-consumption-gpu-findings">Colab Time Consumption (GPU) – Findings</h2>
<ul>
<li>Baseline reference: ~74 minutes for epoch 1 (train ~42m + val ~32m).</li>
<li>With Drive I/O + 4 workers: ~5.25 s/iteration mid-epoch; epochs elongated and inconsistent.</li>
<li>Mitigations: use 2 workers, local dataset copy, regularized setup, or EfficientNet-B0.</li>
</ul>
<h3 id="detailed-training-time-breakdown-resnet50-baseline">Detailed Training Time Breakdown (ResNet50 Baseline)</h3>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 30%" />
<col style="width: 26%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Phase / Epoch</th>
<th style="text-align: right;">Train Duration</th>
<th style="text-align: right;">Val Duration</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Epoch 1</td>
<td style="text-align: right;">~42 min</td>
<td style="text-align: right;">~32 min</td>
<td>Includes initial model + caching overhead</td>
</tr>
<tr class="even">
<td>Epoch 2</td>
<td style="text-align: right;">~5 min</td>
<td style="text-align: right;">~2.3 min</td>
<td>After caching; faster I/O</td>
</tr>
<tr class="odd">
<td>Epoch 3</td>
<td style="text-align: right;">~5 min (train)</td>
<td style="text-align: right;">~2.4 min</td>
<td>Validation degraded; overfitting intensified</td>
</tr>
<tr class="even">
<td>Drive-I/O worst case (observed mid-epoch)</td>
<td style="text-align: right;">&gt;70 min (projected)</td>
<td style="text-align: right;">&gt;30 min (projected)</td>
<td>When streaming directly from Drive with 4 workers and contention</td>
</tr>
</tbody>
</table>
<p><strong>Observation:</strong> After the first epoch warms caches, training becomes much faster if data access is stable. However, reading via Google Drive with high worker count can revert to slow per-iteration times (~5.25s/it). Copying the dataset locally plus reducing workers stabilizes throughput.</p>
<h3 id="test-set-results-best-checkpoint-epoch-1">Test Set Results (Best Checkpoint – Epoch 1)</h3>
<table>
<thead>
<tr class="header">
<th>Metric</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td style="text-align: right;">0.5818</td>
</tr>
<tr class="even">
<td>Macro Precision</td>
<td style="text-align: right;">0.6219</td>
</tr>
<tr class="odd">
<td>Macro Recall</td>
<td style="text-align: right;">0.6152</td>
</tr>
<tr class="even">
<td>Macro F1</td>
<td style="text-align: right;">0.5806</td>
</tr>
<tr class="odd">
<td>ROC AUC (macro)</td>
<td style="text-align: right;">0.6415</td>
</tr>
</tbody>
</table>
<p>Per-Class Detail:</p>
<table>
<thead>
<tr class="header">
<th>Class</th>
<th style="text-align: right;">Precision</th>
<th style="text-align: right;">Recall</th>
<th style="text-align: right;">F1</th>
<th style="text-align: right;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>notdrowsy</td>
<td style="text-align: right;">0.4886</td>
<td style="text-align: right;">0.7877</td>
<td style="text-align: right;">0.6031</td>
<td style="text-align: right;">8,846</td>
</tr>
<tr class="even">
<td>drowsy</td>
<td style="text-align: right;">0.7552</td>
<td style="text-align: right;">0.4427</td>
<td style="text-align: right;">0.5581</td>
<td style="text-align: right;">13,087</td>
</tr>
</tbody>
</table>
<p><strong>Interpretation:</strong> Model favors higher recall for notdrowsy and higher precision for drowsy, indicating a conservative drowsy classification (missed positives) under current training regime.</p>
<h3 id="figure-references">Figure References</h3>
<p><img src="../reports/figures/confusion_matrix_test.png" alt="Confusion Matrix (Test)" /> <img src="../reports/figures/confusion_matrix_test_normalized.png" alt="Confusion Matrix Normalized (Test)" /> <img src="../reports/figures/roc_curves_test.png" alt="ROC Curves (Test)" /> <!-- Optional baseline comparison if generated --> <!-- ![Baseline Comparison](../reports/figures/comparison_baselines.png) --></p>
<hr />
<h2 id="export-to-pdf-options">Export to PDF (Options)</h2>
<ul>
<li>VS Code: Open this file and use “Markdown: Print to PDF” (or the Markdown PDF extension).</li>
<li>Pandoc (if installed):</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode powershell"><code class="sourceCode powershell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pandoc docs<span class="op">/</span>individual<span class="op">-</span>progress<span class="op">-</span>report<span class="op">.</span><span class="fu">md</span> <span class="op">-</span>o docs<span class="op">/</span>individual<span class="op">-</span>progress<span class="op">-</span>report<span class="op">.</span><span class="fu">pdf</span></span></code></pre></div>
<hr />
<h2 id="repository-links">Repository Links</h2>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/hmolhem/nthu-driver-drowsiness-ROI">hmolhem/nthu-driver-drowsiness-ROI</a></li>
<li><strong>Documentation Folder:</strong> <a href="https://github.com/hmolhem/nthu-driver-drowsiness-ROI/tree/main/docs">docs/</a></li>
<li><strong>Evaluation Figures:</strong> <a href="https://github.com/hmolhem/nthu-driver-drowsiness-ROI/tree/main/reports/figures">reports/figures/</a></li>
<li><strong>Training Notebooks:</strong> <a href="https://github.com/hmolhem/nthu-driver-drowsiness-ROI/blob/main/notebooks/colab_gpu_training.ipynb">notebooks/colab_gpu_training.ipynb</a></li>
<li><strong>Latest Release:</strong> <a href="https://github.com/hmolhem/nthu-driver-drowsiness-ROI/releases/tag/v0.1-eval-baseline">v0.1-eval-baseline</a></li>
</ul>
