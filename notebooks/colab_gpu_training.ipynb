{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b25e17",
   "metadata": {},
   "source": [
    "## Quick Resume (Updated Dataset Structure)\n",
    "\n",
    "- Enable GPU (Runtime → Change runtime type → GPU) and run the GPU check.\n",
    "- Mount Drive (ensure /content/drive/MyDrive/datasets contains drowsy/ and notdrowsy/). Use symlink instead of copying.\n",
    "\n",
    "```bash\n",
    "# 1) Clone or pull project\n",
    "%cd /content\n",
    "!git clone https://github.com/hmolhem/nthu-driver-drowsiness-ROI.git || true\n",
    "%cd nthu-driver-drowsiness-ROI\n",
    "!git pull\n",
    "\n",
    "# 2) Install deps (CUDA build)\n",
    "!pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision -U\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# 3) Link dataset (fast, avoids Drive copy)\n",
    "from pathlib import Path\n",
    "root = Path('datasets')\n",
    "drive = Path('/content/drive/MyDrive/datasets')  # Must contain drowsy/, notdrowsy/\n",
    "if root.is_symlink() and root.resolve()==drive:\n",
    "    print('Symlink already exists: datasets ->', drive)\n",
    "elif root.exists() and not root.is_symlink():\n",
    "    print('Local datasets directory exists; using it.')\n",
    "elif drive.exists():\n",
    "    root.symlink_to(drive, target_is_directory=True)\n",
    "    print('✅ Symlink created: datasets -> /content/drive/MyDrive/datasets')\n",
    "else:\n",
    "    raise FileNotFoundError('Missing: /content/drive/MyDrive/datasets')\n",
    "\n",
    "# 4) Patch baseline YAML to Colab-friendly workers\n",
    "import yaml\n",
    "from pathlib import Path as _P\n",
    "p = _P('configs/baseline_resnet50.yaml')\n",
    "cfg = yaml.safe_load(p.read_text()); cfg.setdefault('data', {})\n",
    "cfg['data']['num_workers'] = 2; cfg['data']['data_root'] = 'datasets'; cfg['data']['pin_memory'] = True\n",
    "p.write_text(yaml.safe_dump(cfg, sort_keys=False))\n",
    "print('✅ Updated', p)\n",
    "print('data_root now:', cfg['data']['data_root'])\n",
    "\n",
    "# 5) Train (regularized recommended) and evaluate\n",
    "!python src/training/train_baseline.py --config configs/baseline_resnet50_regularized.yaml --device cuda\n",
    "!python src/eval/evaluate_checkpoint.py --config configs/baseline_resnet50_regularized.yaml --device cuda --save-fig --save-preds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00289f",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b34b0",
   "metadata": {},
   "source": [
    "**Expected output:** Should show GPU info (Tesla T4, L4, or similar)\n",
    "\n",
    "**If you see an error:** Go back and enable GPU runtime!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8ce32",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279e767",
   "metadata": {},
   "source": [
    "**Action required:** Click the link and authorize access to your Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece1852",
   "metadata": {},
   "source": [
    "## Step 3: Clone Project from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/hmolhem/nthu-driver-drowsiness-ROI.git\n",
    "%cd nthu-driver-drowsiness-ROI\n",
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbfb69",
   "metadata": {},
   "source": [
    "**Expected:** Shows \"On branch main\" and clean working tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f0e57",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Dataset Symlink\n",
    "\n",
    "This links your Drive dataset to the project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a34dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link dataset from Drive (run once if symlink missing)\n",
    "%cd /content/nthu-driver-drowsiness-ROI\n",
    "import pathlib\n",
    "root = pathlib.Path('datasets')\n",
    "drive = pathlib.Path('/content/drive/MyDrive/datasets')\n",
    "if root.is_symlink() and root.resolve()==drive:\n",
    "    print('Symlink already exists: datasets ->', drive)\n",
    "elif root.exists() and not root.is_symlink():\n",
    "    print('Local datasets directory present (not symlink); using it.')\n",
    "elif drive.exists():\n",
    "    root.symlink_to(drive, target_is_directory=True)\n",
    "    print('✅ Symlink created: datasets -> /content/drive/MyDrive/datasets')\n",
    "else:\n",
    "    raise FileNotFoundError('Drive dataset not found at /content/drive/MyDrive/datasets')\n",
    "!ls -lh datasets | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3c962",
   "metadata": {},
   "source": [
    "**Expected:** Should list 3 image files from drowsy folder\n",
    "\n",
    "**If error:** Adjust the path in the `ln -s` command to match your Drive structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9de33",
   "metadata": {},
   "source": [
    "## Step 5: Verify Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c05fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch baseline YAML for Colab (workers=2, data_root=datasets)\n",
    "import yaml, pathlib\n",
    "p = pathlib.Path('configs/baseline_resnet50.yaml')\n",
    "cfg = yaml.safe_load(p.read_text())\n",
    "cfg.setdefault('data', {})\n",
    "cfg['data']['num_workers'] = 2\n",
    "cfg['data']['data_root'] = 'datasets'\n",
    "cfg['data']['pin_memory'] = True\n",
    "p.write_text(yaml.safe_dump(cfg, sort_keys=False))\n",
    "print('✅ Updated', p)\n",
    "print('num_workers:', cfg['data']['num_workers'])\n",
    "print('data_root:', cfg['data']['data_root'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0897bbc",
   "metadata": {},
   "source": [
    "**Expected:** Shows train.csv, val.csv, test.csv with sizes and first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47eb684",
   "metadata": {},
   "source": [
    "## Step 6: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbe80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision -U\n",
    "\n",
    "# Install other requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56687c49",
   "metadata": {},
   "source": [
    "**Note:** May see some warnings about version conflicts — these are usually harmless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1967ab",
   "metadata": {},
   "source": [
    "## Step 7: Verify CUDA PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU device:', torch.cuda.get_device_name(0))\n",
    "    print('GPU memory:', torch.cuda.get_device_properties(0).total_memory / 1e9, 'GB')\n",
    "else:\n",
    "    print('WARNING: CUDA not available! Go enable GPU runtime.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4948a9f",
   "metadata": {},
   "source": [
    "**Expected:** \n",
    "- `CUDA available: True`\n",
    "- GPU device: Tesla T4, L4, or similar\n",
    "\n",
    "**If False:** Stop here and enable GPU runtime!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77edb4e",
   "metadata": {},
   "source": [
    "## Step 8: Start Training (ResNet50 Baseline)\n",
    "\n",
    "This will take ~30-60 minutes depending on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with full baseline config (224px, 50 epochs with early stopping)\n",
    "!python src/training/train_baseline.py --config configs/baseline_resnet50.yaml --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c092f",
   "metadata": {},
   "source": [
    "**What to expect:**\n",
    "- Progress bars for each epoch\n",
    "- Training loss should decrease over time\n",
    "- Validation metrics printed after each epoch\n",
    "- Early stopping may trigger before 50 epochs if val metric plateaus\n",
    "- Checkpoints saved to `checkpoints/baseline_resnet50_best.pth` and `_last.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8254606",
   "metadata": {},
   "source": [
    "## Step 9: Verify Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc454f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df025340",
   "metadata": {},
   "source": [
    "**Expected:** Should see `baseline_resnet50_best.pth` and `baseline_resnet50_last.pth` (~90 MB each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fd7c3",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.classifier import create_model\n",
    "from src.data.transforms import get_val_transforms\n",
    "from src.data.dataset import create_dataloaders\n",
    "from src.utils.config import get_config\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load config\n",
    "cfg = get_config(\"configs/baseline_resnet50.yaml\")\n",
    "cfg.data.num_workers = 2  # Colab-friendly\n",
    "\n",
    "# Create test dataloader\n",
    "val_tf = get_val_transforms(cfg.data.image_size)\n",
    "loaders = create_dataloaders(\n",
    "    cfg.data.train_csv, cfg.data.val_csv, cfg.data.test_csv,\n",
    "    data_root=cfg.data.data_root,\n",
    "    train_transform=val_tf,\n",
    "    val_transform=val_tf,\n",
    "    batch_size=cfg.data.batch_size,\n",
    "    num_workers=cfg.data.num_workers\n",
    ")\n",
    "\n",
    "# Load model with best checkpoint\n",
    "model = create_model(cfg)\n",
    "ckpt = torch.load(\"checkpoints/baseline_resnet50_best.pth\", map_location=\"cuda\")\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.cuda().eval()\n",
    "\n",
    "print(\"Best checkpoint from epoch:\", ckpt.get('epoch', 'unknown'))\n",
    "print(\"Best val macro-F1:\", ckpt.get('val_macro_f1', 'unknown'))\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "# Run inference on test set\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels, _ in loaders['test']:\n",
    "        images = images.cuda()\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1).cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(all_labels, all_preds, target_names=['notdrowsy', 'drowsy']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "print(\"\\n[Row: True label | Column: Predicted label]\")\n",
    "print(\"[0=notdrowsy, 1=drowsy]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898ca99",
   "metadata": {},
   "source": [
    "## Step 11: Copy Checkpoints to Drive (for backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backup directory on Drive\n",
    "!mkdir -p /content/drive/MyDrive/drowsiness-results/checkpoints\n",
    "\n",
    "# Copy checkpoints\n",
    "!cp checkpoints/baseline_resnet50_best.pth /content/drive/MyDrive/drowsiness-results/checkpoints/\n",
    "!cp checkpoints/baseline_resnet50_last.pth /content/drive/MyDrive/drowsiness-results/checkpoints/\n",
    "\n",
    "print(\"✅ Checkpoints backed up to Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5f06a",
   "metadata": {},
   "source": [
    "## Step 12: Download Checkpoints (Optional)\n",
    "\n",
    "If you want to download directly to your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write regularized config (updated data_root)\n",
    "from pathlib import Path\n",
    "regularized_yaml = Path('configs/baseline_resnet50_regularized.yaml')\n",
    "regularized_yaml.write_text('''\\\n",
    "model:\n",
    "  name: \"resnet50\"\n",
    "  architecture: \"resnet50\"\n",
    "  pretrained: true\n",
    "  num_classes: 2\n",
    "  freeze_backbone: true\n",
    "  dropout: 0.7\n",
    "\n",
    "data:\n",
    "  data_root: \"datasets\"\n",
    "  train_csv: \"data/splits/train.csv\"\n",
    "  val_csv: \"data/splits/val.csv\"\n",
    "  test_csv: \"data/splits/test.csv\"\n",
    "  image_size: 224\n",
    "  batch_size: 32\n",
    "  num_workers: 2\n",
    "  pin_memory: true\n",
    "\n",
    "augmentation:\n",
    "  enabled: true\n",
    "\n",
    "training:\n",
    "  epochs: 30\n",
    "  optimizer: \"adam\"\n",
    "  learning_rate: 0.00005\n",
    "  weight_decay: 0.0005\n",
    "  lr_scheduler:\n",
    "    type: \"reduce_on_plateau\"\n",
    "    mode: \"max\"\n",
    "    factor: 0.5\n",
    "    patience: 4\n",
    "    min_lr: 0.000001\n",
    "  loss:\n",
    "    type: \"weighted_cross_entropy\"\n",
    "    use_class_weights: true\n",
    "  early_stopping:\n",
    "    enabled: true\n",
    "    patience: 5\n",
    "    monitor: \"val_macro_f1\"\n",
    "    mode: \"max\"\n",
    "  gradient_clipping:\n",
    "    enabled: true\n",
    "    max_norm: 1.0\n",
    "\n",
    "logging:\n",
    "  experiment_name: \"baseline_resnet50_regularized\"\n",
    "  log_dir: \"runs\"\n",
    "  save_dir: \"checkpoints\"\n",
    "  save_best_only: true\n",
    "  save_last: true\n",
    "\n",
    "seed: 42\n",
    "device: \"cuda\"\n",
    "''')\n",
    "print(f'Wrote {regularized_yaml}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11f1bd",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After successful training:\n",
    "\n",
    "1. **Download checkpoints** to your local machine\n",
    "2. **Train EfficientNet-B0 baseline** for comparison:\n",
    "   ```python\n",
    "   !python src/training/train_baseline.py --config configs/baseline_efficientnet.yaml --device cuda\n",
    "   ```\n",
    "3. **Compare results** and create performance tables\n",
    "4. **Begin ROI implementation** (next phase)\n",
    "\n",
    "---\n",
    "\n",
    "**Troubleshooting:**\n",
    "- **CUDA not available:** Enable GPU runtime\n",
    "- **Dataset not found:** Check Drive path in symlink command\n",
    "- **Out of memory:** Reduce batch_size in config\n",
    "- **Import errors:** Re-run pip install cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regularized model (checkpoint evaluator)\n",
    "!python src/eval/evaluate_checkpoint.py --config configs/baseline_resnet50_regularized.yaml --device cuda --save-fig --save-preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Train Regularized Baseline (GPU)\n",
    "!python src/training/train_baseline.py --config configs/baseline_resnet50_regularized.yaml --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5398f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Evaluate Regularized Model on Test Set (metrics + plots)\n",
    "!python src/eval/evaluate_model.py --config configs/baseline_resnet50_regularized.yaml --device cuda --save-fig --save-preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Save & Download Regularized Run Artifacts\n",
    "!zip -r baseline_results_regularized.zip checkpoints/baseline_resnet50_regularized_* runs/baseline_resnet50_regularized/ -q || echo \"zip complete\"\n",
    "from google.colab import files\n",
    "files.download('baseline_results_regularized.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Train EfficientNet-B0 Baseline (GPU)\n",
    "!python src/training/train_baseline.py --config configs/baseline_efficientnet.yaml --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Evaluate EfficientNet-B0 on Test Set (metrics + plots)\n",
    "!python src/eval/evaluate_model.py --config configs/baseline_efficientnet.yaml --device cuda --save-fig --save-preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4df1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Save & Download EfficientNet-B0 Artifacts\n",
    "!zip -r efficientnet_b0_results.zip checkpoints/baseline_efficientnet_b0_* runs/baseline_efficientnet_b0/ -q || echo \"zip complete\"\n",
    "from google.colab import files\n",
    "files.download('efficientnet_b0_results.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b70418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Compare Baselines: ResNet50 vs Regularized vs EfficientNet-B0\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "experiments = [\n",
    "    (\"ResNet50 (baseline)\", Path('runs/baseline_resnet50/metrics_test.json')),\n",
    "    (\"ResNet50 (regularized)\", Path('runs/baseline_resnet50_regularized/metrics_test.json')),\n",
    "    (\"EfficientNet-B0\", Path('runs/baseline_efficientnet_b0/metrics_test.json')),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "missing = []\n",
    "for name, mpath in experiments:\n",
    "    if mpath.exists():\n",
    "        with open(mpath, 'r', encoding='utf-8') as f:\n",
    "            m = json.load(f)\n",
    "        roc_auc = m.get('roc_auc', {})\n",
    "        if isinstance(roc_auc, dict) and roc_auc:\n",
    "            roc_macro = float(np.mean([float(v) for v in roc_auc.values()]))\n",
    "        else:\n",
    "            roc_macro = np.nan\n",
    "        rows.append({\n",
    "            'experiment': name,\n",
    "            'accuracy': float(m.get('accuracy', np.nan)),\n",
    "            'macro_f1': float(m.get('f1_macro', np.nan)),\n",
    "            'precision_macro': float(m.get('precision_macro', np.nan)),\n",
    "            'recall_macro': float(m.get('recall_macro', np.nan)),\n",
    "            'roc_auc_macro': roc_macro,\n",
    "            'source': str(mpath),\n",
    "        })\n",
    "    else:\n",
    "        missing.append((name, str(mpath)))\n",
    "\n",
    "if missing:\n",
    "    print(\"Missing metrics (run those evaluations first):\")\n",
    "    for n, p in missing:\n",
    "        print(f\" - {n}: {p}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if not df.empty:\n",
    "    display(df.sort_values('macro_f1', ascending=False).reset_index(drop=True))\n",
    "    # Plot Macro-F1 and Accuracy\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    order = df.sort_values('macro_f1', ascending=False)['experiment']\n",
    "    sns.barplot(data=df, x='experiment', y='macro_f1', order=order, ax=axes[0], palette='Blues_d')\n",
    "    axes[0].set_title('Macro-F1 (Test)')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].set_ylabel('Macro-F1')\n",
    "    axes[0].tick_params(axis='x', rotation=20)\n",
    "\n",
    "    sns.barplot(data=df, x='experiment', y='accuracy', order=order, ax=axes[1], palette='Greens_d')\n",
    "    axes[1].set_title('Accuracy (Test)')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].set_xlabel('')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].tick_params(axis='x', rotation=20)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_dir = Path('reports/figures')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_png = out_dir / 'comparison_baselines.png'\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    print(f'Saved comparison figure to {out_png}')\n",
    "else:\n",
    "    print('No metrics found to compare yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672d504",
   "metadata": {},
   "source": [
    "## Speed Up: Copy Dataset to Local SSD\n",
    "On Colab, reading from Drive symlinks is slow. Copy the dataset once to the VM's local disk for faster DataLoader performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataset from Drive to local (run once per session)\n",
    "%cd /content/nthu-driver-drowsiness-ROI\n",
    "import os, shutil, pathlib\n",
    "local_root = pathlib.Path('datasets')\n",
    "archive_link = local_root / 'archive'\n",
    "drive_archive = pathlib.Path('/content/drive/MyDrive/datasets/archive')\n",
    "local_root.mkdir(parents=True, exist_ok=True)\n",
    "if archive_link.is_symlink():\n",
    "    archive_link.unlink()\n",
    "if not (local_root / 'archive').exists():\n",
    "    if drive_archive.exists():\n",
    "        print('Copying dataset from Drive to local... (may take several minutes)')\n",
    "        shutil.copytree(drive_archive, local_root / 'archive')\n",
    "        print('✅ Copied to datasets/archive')\n",
    "    else:\n",
    "        raise FileNotFoundError('Drive dataset not found at /content/drive/MyDrive/datasets/archive')\n",
    "else:\n",
    "    print('Local datasets/archive already exists; skipping copy.')\n",
    "!ls -lh datasets | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b75af",
   "metadata": {},
   "source": [
    "## Set DataLoader Workers to 2 (Colab-friendly)\n",
    "Colab often warns about too many workers. This patches the baseline YAML to use 2 workers and the local dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, pathlib\n",
    "p = pathlib.Path('configs/baseline_resnet50.yaml')\n",
    "cfg = yaml.safe_load(p.read_text())\n",
    "cfg.setdefault('data', {})\n",
    "cfg['data']['num_workers'] = 2\n",
    "cfg['data']['data_root'] = 'datasets/archive'\n",
    "cfg['data']['pin_memory'] = True\n",
    "p.write_text(yaml.safe_dump(cfg, sort_keys=False))\n",
    "print('✅ Updated', p)\n",
    "print('num_workers:', cfg['data']['num_workers'])\n",
    "print('data_root:', cfg['data']['data_root'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9440ca3",
   "metadata": {},
   "source": [
    "## Resume Tomorrow: Quick Checklist\n",
    "- Step 1: Enable GPU and run the GPU check.\n",
    "- Step 2: Mount Drive.\n",
    "- Step 3: Clone repo (or `git pull` if it exists).\n",
    "- Step 4: Run the copy-to-local cell to ensure `datasets/archive` exists locally.\n",
    "- Step 5: Ensure `num_workers: 2` in YAML (patch cell above).\n",
    "- Step 6: Start training (regularized config recommended):\n",
    "  - `!python src/training/train_baseline.py --config configs/baseline_resnet50_regularized.yaml --device cuda`\n",
    "- Optional: Evaluate best checkpoint when done:\n",
    "  - `!python src/eval/evaluate_model.py --config configs/baseline_resnet50_regularized.yaml --device cuda --save-fig --save-preds`\n",
    "- Tip: If time is short, try `configs/baseline_efficientnet.yaml` for a lighter model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
